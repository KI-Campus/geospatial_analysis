{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis for Geospatial Application\n",
    "\n",
    "## Lab 2: Logistic Regression classifier & Pytorch basics\n",
    "\n",
    "__Enter your data:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter your names below (double click to edit the table).\n",
    "\n",
    "| Name  | Matr.-nr. |\n",
    "|-|-|\n",
    "| your name | 12345 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In this lab, you will learn how to implement Logistic Regression classifiers using both the Scikit-learn and Pytorch frameworks and how to apply them to randomly generated data samples and existing benchmark datasets for classification purposes.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Please read the instructions for each exercise carefully before answering. You __should not use any external libraries except those that are implemented in the code cell below__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required __libraries__ for this lab can be installed by:\n",
    "\n",
    "__1__. running the following command in a code cell ``!conda install numpy matplotlib scikit-learn`` if not installed already, and \n",
    "\n",
    "__2__. visiting and following the instructions at [pytorch.org](https://pytorch.org/get-started/locally/) to see how __Pytorch__ and __Torchvision__ modules can be installed. The CUDA tab refers to the support of GPUs. Unless you have experience with the CUDA framework select `None` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required __imports__ and __settings__ for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import lab                                      # Given functions\n",
    "import numpy as np                              # Numerical computations\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib                               # Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import torch                                    # PyTorch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 7                                    # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize, PlotSize] \n",
    "CMAP = plt.cm.Accent                            # Color mapping \n",
    "np.set_printoptions(precision=3)                # Array print precision\n",
    "np.random.seed(0)\n",
    "\n",
    "# create the Data folder\n",
    "lab.create_data_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Discriminative Probabilistic Classifiers -  (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use the toy dataset we generated in the first lab. \n",
    "\n",
    "Use the following link [to download the data file](https://seafile.cloud.uni-hannover.de/f/a13a894abe27425ab785/). The file `toy_data.pickle` __should be placed in the `Data` folder__ where this notebook is run. \n",
    "\n",
    "__Run__ the next cell to load the data. \n",
    "\n",
    "If you encounter problems with *pickle* package while loading the file, check the version of *numpy*. You may need to install *numpy==1.24.3* with the command: ``!conda install numpy==1.24.3``. This is because the file was generated by the indicated numpy version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "# LOAD TOY DATASET\n",
    "with open('./Data/toy_data.pickle', 'rb') as file:\n",
    "    Xtoy, ytoy, Xtoy_t, ytoy_t = pickle.load(file)\n",
    "    \n",
    "# Define the training set features, training set labels,  test set features, test set labels\n",
    "X, y, X_t, y_t = Xtoy, ytoy, Xtoy_t, ytoy_t\n",
    "\n",
    "# Check whether the data loading is successfully completed:\n",
    "\n",
    "print(f'Training set features: {X.shape}')\n",
    "print(f'Training set labels: {y.shape}')\n",
    "print(f'Testing set features: {X_t.shape}')\n",
    "print(f'Testing set labels: {y_t.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Logistic regression -  10%\n",
    "\n",
    "In the next cell, starter code for a class implementing Logistic Regression is given. The class design is again adapted from the scikit-learn module. \n",
    "\n",
    "__Complete__ the method ``fit(...)`` that iteratively determines the weights of the classifier.\n",
    "\n",
    "__Note__ that the weights are internally stored in a 2D array (for faster inference). In the missing part of the method you can treat $\\nabla E$ as column vector (like in the lecture). The reshaping is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg():\n",
    "    def __init__(self, num_classes, num_features, sigma=None):\n",
    "        # Initializes the classifier\n",
    "        #  num_classes: number of classes\n",
    "        #  num_features: number of features (length of each feature vector)\n",
    "        #  sigma: regularization term or None for training without regularization\n",
    "        \n",
    "        self.num_classes = M = num_classes\n",
    "        self.num_features_w_bias = F = num_features + 1  # Internaly the bias is handled as additional feature   \n",
    "        self.weights = np.random.uniform(size=(M, F))    # Init. weights using random normally distributed values\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X, y, max_iter=100, crit=0.0001):\n",
    "        # Computes gradient and Hesse matrix for each class using ML\n",
    "        # update the weights using Newton-Raphson method\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        #  y: corresponding labels [num_feature_vectors]\n",
    "        #  max_iter: maximum number of iterations\n",
    "        #  crit: stopping criterion L2 of nabla_E\n",
    "        \n",
    "        X = np.copy(X) # to prevent errors due to modifications of these arrays\n",
    "        y = np.copy(y)\n",
    "        \n",
    "        X = np.hstack((np.ones((X.shape[0],1)), X))      # Add bias feature \n",
    "        N, F = X.shape                                   # N: num_samples\n",
    "        M = self.num_classes                             # M: num_classes\n",
    "\n",
    "        for it in range(max_iter):\n",
    "            nabla_E = np.zeros(((M-1)*F, 1))             # Init. nabla_E\n",
    "            H = np.zeros(((M-1)*F, (M-1)*F))             # Init. H\n",
    "            \n",
    "            ##########################################################\n",
    "            \n",
    "            # YOUR CODE GOES HERE!\n",
    "            #\n",
    "            # 1. Get predictions Y\n",
    "            # 2. Define the Binary indicator as well as the unit matrix I[j,k]\n",
    "            # 3. Update nabla_E and H\n",
    "            # 4. Add regularization if sigma is not None\n",
    "\n",
    "            if not self.sigma is None:\n",
    "                # Add regularization terms to H and nabla_E \n",
    "\n",
    "            ###########################################################\n",
    "            \n",
    "            H_inv = np.linalg.inv(H)                     # Update of weights ..\n",
    "            update = np.dot(H_inv, nabla_E)                 #\n",
    "            self.weights[1:] -= update.reshape(((M-1), F)) \n",
    "            \n",
    "            max2nab = np.sqrt(np.sum(np.square(nabla_E)))   # Check convergence\n",
    "            if max2nab < crit:\n",
    "                break\n",
    "                \n",
    "        print('weights:\\n', self.weights)\n",
    "            \n",
    "\n",
    "    def compute_posteriors(self, X):\n",
    "        # Computes posteriors for feature vectors\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        \n",
    "        num_samples = X.shape[0]                                  # Number of samples to predict\n",
    "        posteriors = np.zeros((num_samples, self.num_classes))    # Init. posterior matrix\n",
    "        \n",
    "        if X.shape[1] < self.num_features_w_bias:                 # Add bias feature (if not done already)\n",
    "            X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "            \n",
    "        for xi, x in enumerate(X):\n",
    "            a = np.dot(self.weights, x)                           # Numerical trick for stability\n",
    "            a -= np.max(a)\n",
    "            eWx = np.exp(a)\n",
    "            posteriors[xi] = eWx/np.sum(eWx)\n",
    "            \n",
    "        return posteriors\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predicts labels for feature vectors in X\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        \n",
    "        P = self.compute_posteriors(X)\n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell a classifier is created and fitted to the training data. __Run the cell__ to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logreg = LogReg(num_classes=4, num_features=2, sigma=5)\n",
    "\n",
    "# Scaling of features for numerical reasons! \n",
    "scale = 1 / np.max(X)\n",
    "\n",
    "# Train the classifier\n",
    "_ = logreg.fit(X * scale, y, max_iter=20)\n",
    "\n",
    "# Create a set of 'all' features in the limits\n",
    "xx, yy = np.meshgrid(np.arange(0, 256, 1), np.arange(0, 256, 1))\n",
    "mesh_features = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Get posteriors for the new test samples\n",
    "PROB = logreg.compute_posteriors(mesh_features * scale)\n",
    "lab.print_probabilities(PROB, (256, 256), 'Posterior', n_cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = logreg.predict(mesh_features * scale)\n",
    "lab.print_decision_boundaries(C, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Logistic regression with feature space transformation -  5%\n",
    "\n",
    "__Implement__ the function ``transform_features`` in the cell below. It takes $X$, a $N\\times2$ feature array, and returns $X2$, the transformed $N\\times5$ feature array. Each row in $X2$ should contain the original features $(x, y)$ as well as the squares $(x^2, y^2)$ and the mixed product $(x\\cdot y)$. For numerical reasons, a scaling of features (scale) needs to be applied to features before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(X, scale):\n",
    "    \n",
    "    # YOUR CODE GOES HERE!\n",
    "    \n",
    "    X2 =\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the following cell to transform the features of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2   = transform_features(X, scale)\n",
    "X2_t = transform_features(X_t, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will train a second classifier based on the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogReg(num_classes=4, num_features=5, sigma=5)\n",
    "logreg2.fit(X2, y)\n",
    "\n",
    "# Get posteriors for the new test samples\n",
    "PROB = logreg2.compute_posteriors(transform_features(mesh_features, scale))\n",
    "lab.print_probabilities(PROB, xx.shape, 'Posterior', n_cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = logreg2.predict(transform_features(mesh_features, scale))\n",
    "lab.print_decision_boundaries(C, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Comparison and evaluation -  10%\n",
    "\n",
    "Use your implementation of the function ``compute_quality_metrics``  from the previous Lab in the cell below.\n",
    "\n",
    "Note that it should computes the following quality metrics (all in the range between 0 and 1):\n",
    "\n",
    "- Precision per class (1D array)\n",
    "- Recall per class (1D array)\n",
    "- F1-score per class (1D array)\n",
    "- Overall accuracy (scalar)\n",
    "- Mean F1-score  (scalar)\n",
    "\n",
    "The function takes the array of predictions $Y$, the corresponding reference labels $y$ and the number of classes $C$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quality_metrics(Y, y, C):\n",
    "\n",
    "    # YOUR CODE GOES HERE!\n",
    "    \n",
    "    precisions = \n",
    "    recalls = \n",
    "    f1_scores = \n",
    "    overall_accuracy = \n",
    "    mean_f1_score = \n",
    "    \n",
    "    return precisions, recalls, f1_scores, overall_accuracy, mean_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell to test your implementation. The code computes and prints the overall accuracy for both datasets and both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, oa_lr_train, mf1_lr_train = compute_quality_metrics(logreg.predict(X * scale), y, 4)\n",
    "_, _, _, oa_lr_test, mf1_lr_test = compute_quality_metrics(logreg.predict(X_t * scale), y_t, 4)\n",
    "\n",
    "_, _, _, oa_lr2_train, mf1_lr2_train = compute_quality_metrics(logreg2.predict(X2), y, 4)\n",
    "_, _, _, oa_lr2_test, mf1_lr2_test = compute_quality_metrics(logreg2.predict(X2_t), y_t, 4)\n",
    "\n",
    "print('Overall Accur. | TRAIN-SET| TEST-SET\\n' + '-' * 37)\n",
    "print('LOG. REG.       |  {:.2%}  |  {:.2%}'.format(oa_lr_train, oa_lr_test))\n",
    "print('LOG. REG. W/ TR.|  {:.2%}  |  {:.2%}'.format(oa_lr2_train, oa_lr2_test))\n",
    "print('\\n')\n",
    "print(' Mean F1-Score  | TRAIN-SET| TEST-SET\\n' + '-' * 37)\n",
    "print('LOG. REG.       |  {:.2%}  |  {:.2%}'.format(mf1_lr_train, mf1_lr_test))\n",
    "print('LOG. REG. W/ TR.|  {:.2%}  |  {:.2%}'.format(mf1_lr2_train, mf1_lr2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write__ a brief discussion, which answers the following questions:\n",
    "\n",
    "- Write a paragraph describing the main steps to train a logistic regression model? This should be answered w.r.t. the methodology, not the implementation.\n",
    "\n",
    "- What is the idea behind the regularization and how does the choice of $\\sigma$ affect the results? Try different values for $\\sigma$,  document and discuss the results. \n",
    "\n",
    "- Explain the idea of the quadratic expansion and the influence on the quality metrics. Would a cubic expansion make sense here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "\n",
    "*Write the discussion here. Do not forget to answer all questions, item by item, and to identify which answer belongs to which question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: PyTorch basics -  (35%)\n",
    "\n",
    "In this part you should get comfortable with the `pytorch` framework.\n",
    "\n",
    "#### Binary toy dataset\n",
    "\n",
    "We start by creating a toy data set for a binary classification problem. Each sample $\\mathbf{x}_i$ consists of two features $x_0$ and $x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 600\n",
    "\n",
    "# Samples for class 0\n",
    "samples_C0 = np.vstack((np.random.randn(num_samples//6, 2) * 0.075 + np.array([[0.75, 0.15]]),  # Samples for class 0\n",
    "                       np.random.randn(num_samples//6, 2) * 0.1 + np.array([[0.7, 0.72]]),\n",
    "                       np.random.randn(num_samples//6, 2) * 0.075 + np.array([[0.25, 0.75]])))\n",
    "# Samples for class 1\n",
    "samples_C1 = np.random.randn(num_samples//2, 2) * 0.15 + np.array([[0.35, 0.4]])                # Samples for class 1\n",
    "\n",
    "all_samples = np.concatenate((samples_C0, samples_C1))                                          # Concat the samples\n",
    "reference_values = np.concatenate((np.zeros(num_samples//2),                                    # Target labels\n",
    "                                   np.ones(num_samples//2))).astype(dtype=np.float32)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize, PlotSize]                                    # Plot samples \n",
    "plt.plot(samples_C0[:,0], samples_C0[:,1], 'b.')\n",
    "plt.plot(samples_C1[:,0], samples_C1[:,1], 'g.')\n",
    "plt.xlim(0, 1); plt.ylim(0, 1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with gradient descent\n",
    "Because the logistic regression model is relatively simple and very close to the concept of neural networks, we start by implementing a logistic regression using the `pytorch` framework. The first step is to convert input samples and reference labels to pytorch tensors, here `X` and`Y`, respectively. The features of torch are mostly comparable to those of numpy. A `torch.tensor` is the fundamental class of the framework, comparable to the `numpy.ndarray` in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(all_samples, dtype=torch.float)  # A tensor can be created from a ndarray, here with explicid datatype\n",
    "Y = torch.tensor(reference_values).float()        # Casting the datatype is also possible with the according functions\n",
    "print(X.shape, Y.shape)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set up the trainable parameters of the model. Remember that the probability $P$ of a feature $x$ for belonging to class 1 is defined as\n",
    "\n",
    "$$P(x = (x_0, x_1)~~|~~C=1) = \\sigma(w_0 \\cdot x_0 + w_1 \\cdot x_1 + b)$$\n",
    "\n",
    "for a binary classification problem. Here, $\\sigma$ is the sigmoid function:\n",
    "\n",
    "$$\\sigma(x) = \\dfrac{1}{1+e^{-x}}$$\n",
    "\n",
    "Three trainable parameters are required here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = torch.tensor([0.0], requires_grad=True)     # All parameters are initialized as a scalar with zero value\n",
    "w_1 = torch.tensor([0.0], requires_grad=True)     # 'requires_grad' enables automatic differentiation\n",
    "b   = torch.tensor([0.0], requires_grad=True)\n",
    "params = [w_0, w_1, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs one update step. __Run the cell multiple times__, to see how the parameters change and the cross entropy loss shrinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Values of w0/w1/b:     {:.4f} / {:.4f} / {:.4f}'.format(        # Print the current values of the parameters.\n",
    "    float(w_0.data), float(w_1.data), float(b.data)))                  # The values can be accessed by the 'data' field\n",
    "\n",
    "P = torch.sigmoid(w_0*X[:,0] + w_1*X[:,1] + b)                         # Compute the output for all samples  \n",
    "P = P.view(-1)                                                         #   and flatten to a 1D tensor\n",
    "\n",
    "E = -1 * torch.mean((Y)*torch.log(P) + (1 - Y)*torch.log(1 - P))       # Computation of the CE loss\n",
    "print('Value of loss E:       {:.5f}'.format(float(E.data)))\n",
    "\n",
    "E.backward()                        # The backward function will compute the gradients for all parameters,\n",
    "                                    #  that the tensor depends on (here, E depends on w_0, w_1 and b)\n",
    "\n",
    "print('Gradients of w0/w1/b:  {:.4f} / {:.4f} / {:.4f}'.format(        # The gradients are stored in the respective \n",
    "    float(w_0.grad.data), float(w_1.grad.data), float(b.grad.data)))   #   tensors and are accessed via 'grad.data'\n",
    "\n",
    "LR = 0.5                            # Define a learning rate and \n",
    "for p in params:\n",
    "    p.data.add_(-LR * p.grad.data)  #  update the parameters (opposite direction of the gradients)\n",
    "    p.grad.data.zero_()             #  each call of 'backward' would accumulate the gradients so we set them to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Training loop -  10%\n",
    "\n",
    "__Complete__ the function `train()` in which the logistic regression model is trained for `num_iter` iterations with gradient descent and a learning rate of `LR`. After training has finished, the function should compute and return the overall accuracy of the last predictions `OA`, a list of all losses `Es` and the final values of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(LR, num_iter):\n",
    "    Es = []                                        # Append cross entropy loss of each iteration to this list\n",
    "    w_0 = torch.tensor([0.0], requires_grad=True)  # Overwrite / initialize the parameters (hard reset)\n",
    "    w_1 = torch.tensor([0.0], requires_grad=True)\n",
    "    b   = torch.tensor([0.0], requires_grad=True)\n",
    "    params = [w_0, w_1, b]\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    f_params = [float(p.data) for p in params]     # Store final parameters as float values\n",
    "    return Es, OA, f_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the classifier is trained with different learning rates. The plot will show the loss over training iterations. Use this as a check for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize//2]  \n",
    "\n",
    "all_f_params = []\n",
    "LRs = [0.5, 0.1, 0.01]\n",
    "for LR in LRs:\n",
    "    Es, OA, f_params = train(LR, 2500)\n",
    "    all_f_params.append(f_params)\n",
    "    print('LR: {}, OA: {:.3%}'.format(LR, OA))\n",
    "    plt.plot(Es, label='LR: {}'.format(LR))\n",
    "    plt.title('Training loss')\n",
    "\n",
    "plt.ylabel(\"CE Loss\"); plt.xlabel(\"Iteration\")\n",
    "plt.legend(); plt.ylim(0, 1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualize__ the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [PlotSize, PlotSize]  \n",
    "\n",
    "plt.plot(samples_C0[:, 0], samples_C0[:, 1], 'b.')\n",
    "plt.plot(samples_C1[:, 0], samples_C1[:, 1], 'g.')\n",
    "\n",
    "x0, x1 = 0.0, 1.0\n",
    "\n",
    "for i, ((w_0i, w_1i, b_i), LR) in enumerate(zip(all_f_params, LRs)):    \n",
    "    y0 = (-x0 * w_0i - b_i) / w_1i\n",
    "    y1 = (-x1 * w_0i - b_i) / w_1i\n",
    "    plt.plot([x0, x1], [y0, y1], label= 'Decision boundary (LR: {})'.format(LR))\n",
    "plt.legend(); plt.xlim(0, 1); plt.ylim(0, 1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Neural networks -  15%\n",
    "\n",
    "Before we start with neural networks, please  read the next two cells __carefully!__ Here, the class `torch.nn.Module` is introduced. It is used to model a network architecture. In the next cell the logistic regression is implemented as such a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):                # A module is created as a class which is derived from torch.nn.Module\n",
    "    \n",
    "    def __init__(self):                 # The __init__ method is implicitly called whenever a new instance is created\n",
    "        super(LogReg, self).__init__()  #  - Parents' __init__ method must be called (passing the current class)\n",
    "        self.fcD = nn.Linear(2, 1)      #  - Defining submodules e.g. dense or convolutional layers\n",
    "                                        #    the `Linear` module creates a dense layer. \n",
    "                                        #    Arguments are number of in / out neurons and \n",
    "                                        #    whether to use a bias or not (default is true)\n",
    "        \n",
    "    def forward(self, x):               # The 'forward' method describes how input is mapped to the output\n",
    "                                        # (It is common to reuse a single variable in sequential networks - here, x)\n",
    "        x = self.fcD(x)                 # First operation: pass input(x) through dense layer\n",
    "        x = torch.sigmoid(x)            # Second operation: apply non-linarity\n",
    "        return x                        # Return the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using torch modules, the computation of the loss as well as the update steps can be simplified. This is shown in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT = 2500         # Hyperparameters: - Number of training iterations (Steps)\n",
    "LR = 0.5          #                  - Learning rate\n",
    "\n",
    "net = LogReg()                                  # Initialize the network\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR)  # Define the optimizer, here SGD: Stochastic Gradient Descent\n",
    "                                                # The optimizer performs the update step and resets gradients\n",
    "\n",
    "Ls = []                         #  We want to keep track of the loss over iterations\n",
    "criterion = nn.BCELoss()        #  Loss models are used to compute loss, here: BCE Binary Cross Entropy \n",
    "\n",
    "for i in range(IT):             # Training loop:\n",
    "    optimizer.zero_grad()       #   Reset gradients\n",
    "    P = net(X)                  #   Feed forward step\n",
    "    P = P.view(-1)              #   Flatten the pridictions to 1D-tensor\n",
    "    loss = criterion(P, Y)      #   Feed predictions and reference to the loss model\n",
    "    loss.backward()             #   Back propagation (compute gradients for all variables)\n",
    "    optimizer.step()            #   Perform gradient descent\n",
    "    Ls.append(float(loss.data)) #   Store loss (converted from tensor to float)\n",
    "\n",
    "predicted_labels = np.round(P.data.numpy())                     # Compute labels by rounding (threshold = 0.5)\n",
    "corrects = np.sum(predicted_labels == Y.data.numpy())           # Count correct predictions\n",
    "print('Overall accuracy: {:.3%}'.format(corrects/num_samples))  # Compute and show overall accuracy\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize//2]             # Plot cross entropy\n",
    "plt.ylabel(\"CE Loss\"); plt.xlabel(\"Iteration\")\n",
    "plt.title('Training loss')\n",
    "plt.plot(Ls, label='Logistic Regression'); plt.ylim(0, 1); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to __visualize__ the decision boundaries of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [0.6*PlotSize, 0.5*PlotSize]  \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, 1, 1/150), np.arange(0, 1, 1/150))\n",
    "grid_samples = np.c_[xx.ravel(), yy.ravel()].reshape(-1,2).astype(np.float32)\n",
    "\n",
    "Z = net(torch.tensor(grid_samples)).data.numpy().reshape(xx.shape)\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=\"seismic\", shading='auto')\n",
    "plt.plot(samples_C0[:, 0]*1, samples_C0[:, 1]*1, 'b.')\n",
    "plt.plot(samples_C1[:, 0]*1, samples_C1[:, 1]*1, 'g.')\n",
    "\n",
    "plt.axis(\"equal\"); plt.colorbar(); plt.xlim(0,1); plt.ylim(0,1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now __implement__ a multilayer perceptron (neural network). The architecture should have:\n",
    "\n",
    "- A variable number of hidden layers (specified by the argument `layers`)\n",
    "- A variable number of neurons in each hidden layer (specified by the argument `neurons`)\n",
    "- Use the `tanh` non-linearity for the hidden layers. (E.g. by using `torch.tanh()`)\n",
    "- Use the `sigmoid` non-linearity for the output layer. \n",
    "\n",
    "Note that for a variable number of layers you have to use either a\n",
    "[ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) or the [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module): \n",
    "    def __init__(self, layers, neurons):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        # YOUR CODE GOES HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE GOES HERE\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop remains similar, but several `settings` are compared. Also training is now done using mini batches. __Add your own new settings__ (minimum 5 settings)  and observe the effect of changing the hyper-parameters of the architecture and the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize//2]  \n",
    "\n",
    "#  The settings are stored as a list of 5-tuples. The parameters are:\n",
    "# (iterations, learning rate, num. layers, num. neurons per layer, batch size) \n",
    "\n",
    "settings = [(3000, 0.5, 2, 8, 64),]  # currently, one example setting; ADD YOUR NEW ONES TO THE LIST\n",
    "\n",
    "for si, (IT, LR, layers, neurons, batch_size) in enumerate(settings):\n",
    "    print(\"setting {}) LR: {}, #Layers: {}, #Neurons: {}, Batch size {}\".format(\n",
    "        si+1, LR, layers, neurons, batch_size))\n",
    "    \n",
    "    Ls = []\n",
    "    criterion = nn.BCELoss()\n",
    "    net = NN(layers, neurons)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=LR)\n",
    "\n",
    "    for i in range(IT):\n",
    "        idx = torch.randperm(X.size(0))[:batch_size]\n",
    "        x, y = X[idx], Y[idx]\n",
    "        optimizer.zero_grad()  \n",
    "        pred = net(x).view(-1)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        Ls.append(float(loss.data))\n",
    "\n",
    "    pred = net(X).view(-1)\n",
    "    predicted_labels = np.round(pred.data.numpy())\n",
    "    corrects = np.sum(predicted_labels == Y.data.numpy())\n",
    "    print('Overall accuracy: {:.3%}'.format(corrects/num_samples))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(Ls); plt.ylim(0, 1); plt.ylabel(\"CE Loss\"); plt.xlabel(\"Iteration\")\n",
    "\n",
    "    Z = net(torch.tensor(grid_samples)).data.numpy().reshape(xx.shape)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=\"seismic\", shading='auto')\n",
    "    plt.plot(samples_C0[:, 0]*1, samples_C0[:, 1]*1, 'b.')\n",
    "    plt.plot(samples_C1[:, 0]*1, samples_C1[:, 1]*1, 'g.')\n",
    "\n",
    "    plt.axis(\"equal\"); plt.xlim(0,1); plt.ylim(0,1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3:  Discussion -  10%\n",
    "\n",
    "- Write a discussion based on the settings in the previous cell. Run several settings and discuss the influence of the hyper-parameters: learning rate, number of layers, number of neurons per layer and batch size. A recommended strategy is to use one baseline setting and then add scenarios where one hyper-parameter is changed, respectively. For each hyper-parameter argue with what you observe in the experiments but also from a theoretical viewpoint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "\n",
    "*Write the discussion here. Do not forget to answer all questions, item by item, and to identify which answer belongs to which question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Image classification -  (40%)\n",
    "\n",
    "In the previous exercise, we have learned how to implement a perceptron (Logistic Regression) and a Multilayer Perceptron for binary classification using a toy dataset. In this exercise, we are going to apply the same models for classifying images using the __pytorch__ framework on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. It contains images of hand written digits. Each image has 28 x 28 pixels and one channel. The classes correspond to the digits 0-9. The given function `lab.MnistGenerator()` will download the dataset (if not already available). The images in the MNIST dataset are already split into:\n",
    "\n",
    "- 60.000 images for training\n",
    "- 10.000 images for testing\n",
    "\n",
    "We will preserve 1.000 training images as a validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualize few data samples from training. You can run the cell several times to see some digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = lab.MnistGenerator()\n",
    "Xs, Ys = gen.get_train_batch()\n",
    "\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 3, 3\n",
    "print('Examples of digits:')\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(Xs), size=(1,)).item()\n",
    "    img, label = Xs[sample_idx], Ys[sample_idx]\n",
    "    title = \"Class {}:\".format(label)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Defining the models -  15%\n",
    "\n",
    "We will train two different models on the dataset:\n",
    "\n",
    "__1. Logistic regression:__\n",
    "\n",
    "- No hidden layer, direct mapping from (normalized) input to output.\n",
    "\n",
    "__2. Multilayer perceptron:__\n",
    "\n",
    "- One hidden layer with 256 neurons and tanh non-linearity.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- In each forward pass __normalize__ the images to a range of -1 to 1 (input values are in range 0 to 255).\n",
    "- Use `x=x.view(-1, N)` to __flatten__ a feature map (with `N` elements). This is required when passing images or feature maps to dense layers.\n",
    "- In all two cases __the last layer must not have a non-linearity__ (because the softmax is included in the loss formulation we will use later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "The training loop is given in the next cell. It is similar to the previous training loop, but additionally, an evaluation on the validation is performed.\n",
    "\n",
    "__Complete__ the function to include the evaluation on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(gen, net, optimizer, num_iter):\n",
    "    Ls = []\n",
    "    TBAs = []\n",
    "    VAs = []\n",
    "    \n",
    "    net.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        X, Y = gen.get_train_batch()\n",
    "        num_samples = float(len(Y))\n",
    "        \n",
    "        TX = torch.tensor(X).view(-1, 1, 28, 28)\n",
    "        TY = torch.tensor(Y).long()\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        pred = net(TX)\n",
    "        loss = criterion(pred, TY)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        Vs, Is = torch.max(pred, -1)\n",
    "        num_correct = np.sum(Is.data.numpy() == Y)\n",
    "        TBA = num_correct/num_samples\n",
    "        print('\\rIt {}/{}, CE: {:.3f}, Batch Acc.: {}/{} = {:.1%}'.format(\n",
    "            i, num_iter, loss.data, num_correct, len(Y), TBA), end='')\n",
    "        \n",
    "        # Compute accuracy of validation set\n",
    "        \n",
    "        X, Y = gen.get_validation_batch()\n",
    "        num_samples = float(len(Y))\n",
    "        TX = torch.tensor(X).view(-1, 1, 28, 28)\n",
    "        TY = torch.tensor(Y).long()\n",
    "        net.eval()\n",
    "        pred = net(TX)\n",
    "        net.train()\n",
    "        Vs, Is = torch.max(pred, -1)\n",
    "        num_correct = np.sum(Is.data.numpy() == Y)\n",
    "        VA = num_correct/num_samples\n",
    "        \n",
    "        Ls.append(float(loss.data))\n",
    "        TBAs.append(TBA)\n",
    "        VAs.append(VA)\n",
    "        \n",
    "    # Compute accuracy of test set\n",
    "        \n",
    "    X, Y = gen.get_test_batch()\n",
    "    \n",
    "    # ENTER YOUR CODE HERE\n",
    "    #\n",
    "    # 1. Convert X and Y into Tensors\n",
    "    # 2. Set the network into inference mode\n",
    "    # 3. Make predictions\n",
    "    \n",
    "    CM = confusion_matrix(Y, Is)\n",
    "    return Ls, TBAs, VAs, CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Training -  10%\n",
    "\n",
    "In this section the models are trained (with different settings).\n",
    "In each cell, __choose appropriate hyperparameters__ to achieve stable results. In particular choose:\n",
    "\n",
    "- Batch size `BS`\n",
    "- Number of iterations `IT`\n",
    "- Learning rate `LR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = \n",
    "LR = \n",
    "IT = \n",
    "\n",
    "gen = lab.MnistGenerator(BS) # Data generator\n",
    "net = LogReg() \n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR)\n",
    "\n",
    "Ls, TBAs, VAs, CM = train_network(gen, net, optimizer, IT)\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize//2]\n",
    "lab.print_summary(Ls, TBAs, VAs, CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the logistic regression has no intermediate layers, the the learned weights can be  interpreted visually. The following code will show the feature maps as images. You should be able to see patterns that correspond to the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is = []\n",
    "weights = net.fc1.weight.data.numpy()\n",
    "for i in range(10):\n",
    "    Is.append(weights[i].reshape((28,28)))\n",
    "    Is.append(np.ones((28,1))*np.min(weights))\n",
    "I = np.hstack(Is)\n",
    "plt.imshow(I, cmap='copper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Multilayer perceptron (Neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = \n",
    "LR = \n",
    "IT =\n",
    "\n",
    "gen = lab.MnistGenerator(BS)\n",
    "net = MLP() \n",
    "optimizer = optim.SGD(net.parameters(), lr=LR)\n",
    "\n",
    "Ls, TBAs, VAs, CM = train_network(gen, net, optimizer, IT)\n",
    "lab.print_summary(Ls, TBAs, VAs, CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Discussion  -  15%\n",
    "\n",
    "__Write a discussion__ which answers the following aspects and questions: \n",
    "\n",
    "- During all experiments, what was the main influence of learning rate and batch size?\n",
    "\n",
    "- Discuss the performance of both models (LogReg and MLP). What might be the reasons behind the differences in the performance you observe?\n",
    "\n",
    "- Describe existing techniques that can be used to improve the classification results of a MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "\n",
    "*Write the discussion here. Do not forget to answer all questions, item by item, and to identify which answer belongs to which question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
