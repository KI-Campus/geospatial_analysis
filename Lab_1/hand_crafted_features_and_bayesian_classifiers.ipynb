{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MU07URavY4j"
   },
   "source": [
    "#  Image Analysis for Geospatial Application\n",
    "\n",
    "##  Lab 1: Hand-crafted features & Bayesian classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjF6msCEvY4l"
   },
   "source": [
    "Please enter your names below (double click to edit the table).\n",
    "\n",
    "| Name  | Matr.-nr. |\n",
    "|-|-|\n",
    "| your name | 12345 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In this lab, first you will implement methods to extract features from images, implement and train Bayesian classifiers to classify real images using the extracted features. After that, you will apply the classifiers to a synthetic toy dataset in a 2D feature space.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __libraries__ required for this lab can be installed by running the following command in a code cell:\n",
    "\n",
    "``!conda install numpy matplotlib scikit-learn imageio``\n",
    "\n",
    "__Note that this only needs to be done if you have not installed these libraries already.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBhcutSxvY4m"
   },
   "source": [
    "Required __imports__ for this lab. Do not forget to run the following cell (which may __take up to some minutes__ because some functions are compiled)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Please read the instructions for each exercise carefully before answering. You __should not use any external libraries except those that are implemented in the code cell below__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following link [to download the data](https://seafile.cloud.uni-hannover.de/d/add5ddfba5bf43699f1b/). The folder `Vaihingen` contains both training and test samples and __should be placed in the `Data` folder__ where this notebook is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import lab                                      # Given functions\n",
    "import numpy as np                              # Numerical computations\n",
    "import matplotlib                               # Plots\n",
    "import matplotlib.pyplot as plt     \n",
    "import imageio.v2 as imageio\n",
    "import tifffile                                 # Reading .tiff file\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 8                                     # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize]  \n",
    "CMAP = plt.cm.Accent                             # Color mapping \n",
    "np.set_printoptions(precision=3)                 # Array print precision\n",
    "\n",
    "# CLASS AND FEATURE DESCRIPTION\n",
    "class_names   = ['STREET','HOUSE','LOW VEG.','HIGH VEG.','CAR']\n",
    "feature_names = ['NDVI','NDSM','NIR','RED','GREEN']\n",
    "num_classes   = len(class_names); num_features = len(feature_names)\n",
    "\n",
    "training_set_path = './Data/Vaihingen/Train/'     # Relative path to training patch root folder\n",
    "test_set_path     = './Data/Vaihingen/Test/'      # Relative path to test patch root folder\n",
    "\n",
    "\n",
    "# create the Data folder if it does not exist\n",
    "lab.create_data_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpJe1LhKvY4n"
   },
   "source": [
    "## Exercise 1: Feature extraction & Data preparation -  (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGhyG6HDvY4s"
   },
   "source": [
    "### Exercise 1.1: Normalized Difference Vegetation Index (NDVI) -  5%\n",
    "\n",
    "__Implement__ the function below, which computes a NDVI image. You can assume that the first channel of the input image corresponds to the near infrared band ($NIR$) and the second channel corresponds to the red band ($R$). The $NDVI$ is defined as\n",
    "\n",
    "$$ NDVI = \\dfrac{NIR-R}{NIR+R} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lGZnKZavY4s"
   },
   "outputs": [],
   "source": [
    "def compute_ndvi(I):\n",
    "    h, w, d = I.shape\n",
    "    assert d == 3, \"ndvi computation only valid on multi channel images!\"\n",
    "    num_samples = h*w\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    #\n",
    "    # Step 1: compute the NDVI channel from the first and second channels of Image\n",
    "    # step 2: prevent a possible division by zero\n",
    "    \n",
    "    return NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq21itBavY4s",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "I = lab.imread3D('images/house_nirrg.jpg')\n",
    "\n",
    "# apply your function on the Image\n",
    "NDVI = compute_ndvi(I)\n",
    "\n",
    "# compute normalized NDVI-image\n",
    "NDVI = lab.normalize(NDVI.reshape((I.shape[0], I.shape[1], 1)))\n",
    "\n",
    "lab.imshow3D(I)\n",
    "print('Original Image')\n",
    "\n",
    "lab.imshow3D(NDVI)           \n",
    "print('Normalized Difference Vegetation Index (normalized)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Data pre-processing -  15%\n",
    "\n",
    "#### Data overview\n",
    "\n",
    "In this exercise aerial images will be classified. There is one patch for training and one for testing. Each patch has a size of $800px \\times 800 px$. For each pixel the values for\n",
    "\n",
    "- Near infra red (NIR)\n",
    "- Red (R)\n",
    "- Green (G)\n",
    "\n",
    "are available in the image ('IR_R_G.png'), where the first channel corresponds to NIR, the second to Red and the third to Green. The values are quantized with 8 bits (1 byte) per pixel and channel. Additionally, a normalized digital surface model (NDSM) is available. The information is stored in an image having 32 bits per pixel ('NDSM.tif'), each pixel containing the height above ground in [m] as a `float32` value. The ground truth (GT) labels are given in a colour coded image file ('GT.png'), in which colours correspond to class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next two cells to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data')\n",
    "lab.plot_patch(training_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test data')\n",
    "lab.plot_patch(test_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implement__ the function `read_patch(root_folder)`. The function should:\n",
    "\n",
    "- Read IR_R_G, NDSM and GT images located inside the `root_folder`, using the filenames given above\n",
    "- Change data type of IR, R and G to `float32`\n",
    "- Compute the NDVI as an additional feature (call the function you implemented in Exercise 1.1)\n",
    "- Shift and scale IR, R and G so that 0 will be mapped to -1.0 and 255 will be mapped to 1.0 \n",
    "- Normalize the NDSM by subtracting 5.0 before dividing by 5.0.\n",
    "- Build the GT label maps using the given color codes. __The classes clutter and low vegetation should be merged!__\n",
    "\n",
    "The function should return two arrays $X, y$, where $X$ is an $N\\times 5$ matrix and $y$ is an $N$-dimensional vector holding the true labels. $N$ is the number of samples ($N_{rows}\\times N_{cols}$, i.e. the number of rows times the number columns of the images; here, we have $N_{rows} = N_{cols} = 800$). Each row in $X$ should contain the normalized features in the order (NIR, R, G, NDSM, NDVI).\n",
    "\n",
    "__Colour codes of the classes:__\n",
    "\n",
    "|Class ID|Description|Colour|Value|\n",
    "|-|-|-|-|\n",
    "|0|Street|WHITE|(255,255,255)|\n",
    "|1|Building|BLUE|(0,0,255)|\n",
    "|2|Low Vegetation|CYAN|(0,255,255)|\n",
    "|3|High Vegetation|GREEN|(0,255,0)|\n",
    "|4|Car|YELLOW|(255,255,0)|\n",
    "|2|Clutter|RED|(255,0,0)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patch(root_folder):\n",
    "    \n",
    "    # Read images\n",
    "    # Convert IRRG to float32\n",
    "    # Compute NDVI: call compute_ndvi(); your previous implementation\n",
    "    # Shift and scale 'IRRG' \n",
    "    # Shift and scale NDSM by using the instructions given in the above description\n",
    "    # Stack features to 'X'\n",
    "    # Save labels to 'y'\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell to read the training and testing patch. The assertions will check the shape of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_patch(training_set_path)\n",
    "X_t, y_t = read_patch(test_set_path)\n",
    "\n",
    "assert X.shape == (800*800, 5), \"X has a wrong shape\"\n",
    "assert y.shape == (800*800,),   \"y has a wrong shape\"\n",
    "\n",
    "print('min/max irrg', np.min(X_t[:,:3]), np.max(X_t[:,:3]))\n",
    "print('min/max ndsm', np.min(X_t[:, 3]), np.max(X_t[:, 3]))\n",
    "print('min/max ndvi', np.min(X_t[:, 4]), np.max(X_t[:, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "In the next cell, use your previous implementation of the function `read_patch()` to select only two features: __NDSM__ and __NDVI__. The `_t` used in variables defined below stands for test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y     = read_patch(training_set_path) # training\n",
    "X_t, y_t = read_patch(test_set_path)     # testing\n",
    " \n",
    "# YOUR CODE GOES HERE!\n",
    "\n",
    "ndsm, ndsm_t, ndvi, ndvi_t = #\n",
    "\n",
    "# New training and test sample sets\n",
    "X   = np.hstack((ndsm, ndvi))\n",
    "X_t = np.hstack((ndsm_t, ndvi_t))\n",
    "\n",
    "# The assertions will again check the shape of the result.\n",
    "assert X.shape == (800*800, 2), \"X has a wrong shape\"\n",
    "assert y.shape == (800*800,),   \"y has a wrong shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Evaluation -  (10%)\n",
    "\n",
    "For a visual evaluation, __implement__ the function `get_labels_as_image()` that turns a label vector $Y$ back into an image with height, width and depth as defined in `shape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_as_image(Y, shape):\n",
    "    # Convert labels Y back to a color image\n",
    "    \n",
    "    \n",
    "    return Y_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell will check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YC = np.sum(imageio.imread(training_set_path + 'GT.png')*(1,3,5),-1)\n",
    "YP = np.sum(get_labels_as_image(y, [800,800,3])*(1,3,5),-1)\n",
    "if np.equal(YC[YC>255], YP[YC>255]).all():\n",
    "    print('implementation seems correct!')\n",
    "else:\n",
    "    print('implementation seems wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the classifiers quantitatively, we need a function that computes quality metrics. \n",
    "__Implement__ the function below, that computes the following metrics (all in range 0 - 1):\n",
    "\n",
    "- Precision per class (1D array)\n",
    "- Recall per class (1D array)\n",
    "- F1-score per class (1D array)\n",
    "- Overall accuracy (scalar)\n",
    "- Mean F1-score  (scalar)\n",
    "\n",
    "The function takes the array of predictions $Y$, the corresponding reference labels $y$ and the number of classes $C$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quality_metrics(Y, y, C):\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    return precisions, recalls, f1_scores, overall_accuracy, mean_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6FZ7RmUvY4t"
   },
   "source": [
    "## Exercise 3: Generative probabilistic classifiers -  (35%)\n",
    "\n",
    "### Exercise 3.1: Single Gaussian Model -  10%\n",
    "\n",
    "In the following cell, some code for a Bayesian classifier using a single Gaussian Model for the likelihood (Normal Distribution Classifier, NDC) is given. The class design is adapted from the module scikit-learn. Following that implementation each classifier is implemented as a class. The method `fit(X, y)` takes the training samples $X$ and the corresponding labels $y$ as arguments and fits the model to the data. For the NDC this means to compute the mean and covariance for each class. The function `compute_likelihoods(X)` computes the likelihoods for the given feature vectors $X$ to belong to each of the classes.\n",
    "\n",
    "__Complete__ the method ``fit(X, y)`` that computes the mean and the covariance matrix for each class.\n",
    "\n",
    "__Complete__ the method ``compute_likelihoods(X)`` that computes the likelihood of the features `X`.\n",
    "\n",
    "__Complete__ the method ``compute_posteriors(L, P_prior)`` that computes the posterior probabilities $P_{post}$ for $N$ samples. Inputs are the likelihood tensor $L$ with shape $N\\times c$ for $N$ samples and $c$ classes and the prior probabilities $P_{prior}$ for the classes. Remember, $P_{prior}$ is a one dimensional vector that contains the prior probabilities for each class: $P_{prior} = [p(C_0), ... ,p(C_c)]$. Make sure, not to modify $L$ in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDistributionClassifier():\n",
    "    \n",
    "    def __init__(self, num_classes, num_features):\n",
    "        #  Initializes the classifier\n",
    "        #  num_classes: number of classes\n",
    "        #  num_features: number of features (length of each feature vector)\n",
    "        \n",
    "        C = num_classes\n",
    "        F = num_features\n",
    "        self.num_classes = C                    # Store number of classes  \n",
    "        self.num_features = F                   # Store number of features \n",
    "        self.means = np.zeros((C, F))           # Init. means\n",
    "        self.covars = np.zeros((C, F, F))       # Init. covars\n",
    "        self.inv_covars = np.zeros((C, F, F))   # Init. inverses of covars\n",
    "        self.det_covars = np.zeros((C))         # Init. determinants\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #  Computes mean and covariance matrix for each class using ML\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        #  y: corresponding labels [num_feature_vectors]\n",
    "        \n",
    "        feature_vectors_by_classes = lab.get_feature_vectors_by_classes(X, y) # Group samples by classes\n",
    "        for c in range(self.num_classes):\n",
    "            feature_vecs_class_c = feature_vectors_by_classes[c]\n",
    "            ########################################################## \n",
    "            \n",
    "            # YOUR CODE GOES HERE!\n",
    "            \n",
    "            self.means[c] =  # mean for class c\n",
    "            self.covars[c] = # covariance matrix for class c\n",
    "            \n",
    "            ###########################################################\n",
    "            \n",
    "            self.inv_covars[c] = np.linalg.inv(self.covars[c])\n",
    "            self.det_covars[c] = np.linalg.det(self.covars[c])\n",
    "\n",
    "    def uniform_prior(self):\n",
    "        # Assumes uniform priors\n",
    "        \n",
    "        return [1/self.num_classes for i in range(self.num_classes)]\n",
    "            \n",
    "    def compute_likelihoods(self, X):\n",
    "        #  Computes likelihoods for feature vectors\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        \n",
    "        num_samples = X.shape[0]                                  # Number of samples to predict\n",
    "        likelihoods = np.zeros((num_samples, self.num_classes))   # Init. likelihood matrix\n",
    "        \n",
    "        for xi, x in enumerate(X):\n",
    "            for c in range(self.num_classes):\n",
    "                ########################################################## \n",
    "                \n",
    "                # YOUR CODE GOES HERE!\n",
    "            \n",
    "                likelihoods[xi, c] = \n",
    "                ##########################################################\n",
    "                \n",
    "        return likelihoods\n",
    "\n",
    "    \n",
    "    def compute_posteriors(self, L, P_prior=None):\n",
    "        # Computes posteriors for feature vectors\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        \n",
    "        # If the P_prior is None, we assume a uniform prior. This will help us\n",
    "        # to call the function .predict() without specifiying priors beforehand.\n",
    "        if P_prior is None:\n",
    "            P_prior = self.uniform_prior()\n",
    "        assert np.sum(P_prior) == 1.0, \"The prior has to sum up to one!\"\n",
    "        ##########################################################\n",
    "        \n",
    "        # YOUR CODE GOES HERE!\n",
    "        \n",
    "        posteriors = \n",
    "        \n",
    "        ##########################################################\n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Predicts labels for feature vectors in X\n",
    "        #  X: feature_vectors [num_feature_vectors x num_features]\n",
    "        \n",
    "        P = self.compute_posteriors(self.compute_likelihoods(X))\n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell a classifier is created and trained. __Run the cell__ to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier instance\n",
    "ndc = NormalDistributionClassifier(num_classes=num_classes, num_features=2)\n",
    "\n",
    "# Train the classifier\n",
    "ndc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells will compute the predictions by assuming a uniform prior and run the qualitative and visual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "# Compute likelihoods\n",
    "L_Y_t = ndc.compute_likelihoods(X_t)  \n",
    "\n",
    "# Compute posteriors assuming uniform prior\n",
    "P_Y_t = ndc.compute_posteriors(L_Y_t, [0.20,0.20,0.20,0.20,0.20])\n",
    "Y_t = np.argmax(P_Y_t, axis=1)\n",
    "\n",
    "I_pred = get_labels_as_image(Y_t, (800, 800, 3))\n",
    "I_gt   = get_labels_as_image(y_t, (800, 800, 3))\n",
    "\n",
    "lab.plot_pred_gt(I_pred, I_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, f1_scores, overall_accuracy, mean_f1_score = compute_quality_metrics(Y_t, y_t, 5)\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('F1-score   [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean F1-score   : {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation of the NDC with multiple features\n",
    "\n",
    "In the next cell, consider adding more features (NIR, R, G) to the training and test data, then compute the predictions by assuming a uniform prior and run the qualitative and visual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 5 features (NIR, R, G, NDSM, NDVI)\n",
    "\n",
    "X_5, y_5 = read_patch(training_set_path)\n",
    "X_t_5, y_t_5 = read_patch(test_set_path)\n",
    "\n",
    "# Create classifier instance\n",
    "ndc_5 = NormalDistributionClassifier(num_classes=num_classes, num_features=5)\n",
    "\n",
    "# Train the classifier:\n",
    "ndc_5.fit(X_5, y_5)   \n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Compute likelihoods\n",
    "L_Y_t_5 = ndc_5.compute_likelihoods(X_t_5)  \n",
    "\n",
    "# Compute posteriors assuming uniform prior\n",
    "P_Y_t_5 = ndc_5.compute_posteriors(L_Y_t_5, [0.20,0.20,0.20,0.20,0.20])\n",
    "Y_t_5   = np.argmax(P_Y_t_5, axis=1)\n",
    "\n",
    "I_pred  = get_labels_as_image(Y_t_5, (800, 800, 3))\n",
    "I_gt    = get_labels_as_image(y_t_5, (800, 800, 3))\n",
    "\n",
    "lab.plot_pred_gt(I_pred, I_gt)\n",
    "\n",
    "precisions, recalls, f1_scores, overall_accuracy, mean_f1_score = compute_quality_metrics(Y_t_5, y_t_5, 5)\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('F1-score   [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean F1-score   : {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Mixture of Gaussian Model -  5%\n",
    "\n",
    "The next cell will fit a Gaussian Mixture Model (GMM) to the samples of each class. The list `N_clusters` defines the number of components per class. __Modify__ the list in a meaningful way by assuming TWO components per class  with a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_clusters = []  # To be modified!\n",
    "\n",
    "# Instantiating the classifier\n",
    "gmm = lab.GaussianMixtureClassifier(N_clusters)\n",
    "\n",
    "# Train the classifier using EM:\n",
    "gmm.fit(X, y)   \n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Compute likelihoods\n",
    "L_Y_t = gmm.compute_likelihoods(X_t)  \n",
    "\n",
    "# Compute posteriors assuming uniform prior\n",
    "P_Y_t = gmm.likelihoods_to_posteriors(L_Y_t, [0.20,0.20,0.20,0.20,0.20])\n",
    "Y_t = np.argmax(P_Y_t, axis=1)\n",
    "\n",
    "I_pred = get_labels_as_image(Y_t, (800, 800, 3))\n",
    "I_gt   = get_labels_as_image(y_t, (800, 800, 3))\n",
    "\n",
    "# Plot predictions\n",
    "lab.plot_pred_gt(I_pred, I_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell for qualitative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, f1_scores, overall_accuracy, mean_f1_score = compute_quality_metrics(Y_t, y_t, 5)\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('F1-scores  [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean F1-score   : {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a GMM with multiple features\n",
    "\n",
    "In the next setup, consider adding more features (NIR, R, G) to the training and test samples, then compute the predictions by assuming a uniform prior and run the qualitative and visual evaluation. You can use the same setting for `N_clusters` as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 5 features (NIR, R, G, NDSM, NDIV)\n",
    "\n",
    "X_5, y_5 = read_patch(training_set_path)\n",
    "X_t_5, y_t_5 = read_patch(test_set_path)\n",
    "\n",
    "# Set similar number of components as in previous setting for each class\n",
    "N_clusters = [] # To be set! \n",
    "\n",
    "# Instantiating the classifier\n",
    "gmm_5 = lab.GaussianMixtureClassifier(N_clusters)\n",
    "\n",
    "# Train the classifier using EM:\n",
    "gmm_5.fit(X_5, y_5)   \n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Compute likelihoods\n",
    "L_Y_t_5 = gmm_5.compute_likelihoods(X_t_5)  \n",
    "\n",
    "# Compute posteriors assuming uniform prior\n",
    "P_Y_t_5 = gmm_5.likelihoods_to_posteriors(L_Y_t_5, [0.20,0.20,0.20,0.20,0.20])\n",
    "Y_t_5   = np.argmax(P_Y_t_5, axis=1)\n",
    "\n",
    "I_pred = get_labels_as_image(Y_t_5, (800, 800, 3))\n",
    "I_gt   = get_labels_as_image(y_t_5, (800, 800, 3))\n",
    "\n",
    "lab.plot_pred_gt(I_pred, I_gt)\n",
    "\n",
    "precisions, recalls, f1_scores, overall_accuracy, mean_f1_score = compute_quality_metrics(Y_t_5, y_t_5, 5)\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('F1-score   [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean F1-score   : {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Comparison and evaluation -  20%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell will compute and print the overall accuracy for both datasets and both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, oa_ndc_train, mf1_ndc_train = compute_quality_metrics(ndc.predict(X), y, 5)\n",
    "_, _, _, oa_ndc_test, mf1_ndc_test = compute_quality_metrics(ndc.predict(X_t), y_t, 5)\n",
    "\n",
    "_, _, _, oa_gmm_train, mf1_gmm_train = compute_quality_metrics(gmm.predict(X), y, 5)\n",
    "_, _, _, oa_gmm_test, mf1_gmm_test = compute_quality_metrics(gmm.predict(X_t), y_t, 5)\n",
    "\n",
    "_, _, _, oa_ndc_5_train, mf1_ndc_5_train = compute_quality_metrics(ndc_5.predict(X_5), y_5, 5)\n",
    "_, _, _, oa_ndc_5_test, mf1_ndc_5_test = compute_quality_metrics(ndc_5.predict(X_t_5), y_t_5, 5)\n",
    "\n",
    "_, _, _, oa_gmm_5_train, mf1_gmm_5_train = compute_quality_metrics(gmm_5.predict(X_5), y_5, 5)\n",
    "_, _, _, oa_gmm_5_test, mf1_gmm_5_test = compute_quality_metrics(gmm_5.predict(X_t_5), y_t_5, 5)\n",
    "\n",
    "print('Overall Accur. | TRAIN-SET| TEST-SET\\n' + '-' * 37)\n",
    "print('SINGLE GAUSSIAN                     |  {:.2%}  |  {:.2%}'.format(oa_ndc_train, oa_ndc_test))\n",
    "print('SINGLE GAUSSIAN WITH MULT. FEATURES |  {:.2%}  |  {:.2%}'.format(oa_ndc_5_train, oa_ndc_5_test))\n",
    "print('MIXTURE OF GAU.                     |  {:.2%}  |  {:.2%}'.format(oa_gmm_train, oa_gmm_test))\n",
    "print('MIXTURE OF GAU. WITH MULT. FEATURES |  {:.2%}  |  {:.2%}'.format(oa_gmm_5_train, oa_gmm_5_test))\n",
    "print('\\n')\n",
    "print('Mean F1-Score  | TRAIN-SET| TEST-SET\\n' + '-' * 37)\n",
    "print('SINGLE GAUSSIAN                     |  {:.2%}  |  {:.2%}'.format(mf1_ndc_train, mf1_ndc_test))\n",
    "print('SINGLE GAUSSIAN WITH MULT. FEATURES |  {:.2%}  |  {:.2%}'.format(mf1_ndc_5_train, mf1_ndc_5_test))\n",
    "print('MIXTURE OF GAU.                     |  {:.2%}  |  {:.2%}'.format(mf1_gmm_train, mf1_gmm_test))\n",
    "print('MIXTURE OF GAU. WITH MULT. FEATURES |  {:.2%}  |  {:.2%}'.format(mf1_gmm_5_train, mf1_gmm_5_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a discussion__ which __briefly__ answers the following questions: \n",
    "\n",
    "- Describe the parameters of a single Gaussian model and a Gaussian mixture model and how they are determined in the training process.\n",
    "\n",
    "- How is an unseen feature vector classified (both models)?\n",
    "\n",
    "- Compare the overall accuracy to the class-specific metrics of all variant of bayesian classifiers implemented in this experiment. Which problem can be observed? Why do you think this problem occurs and what could be done to avoid it?\n",
    "\n",
    "- Based on the classification results (performance of both models on both training-test datasets), did the classification improve when using a single Gaussian model (w.r.t. the result of the mixture of Gaussian model)? Why / why not? What can be observed when using multiple features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "*Write the discussion here. Do not forget to answer all questions, item by item, and to identify which answer belongs to which question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Application to a synthetic toy dataset -  (35%)\n",
    "\n",
    "### Exercise 4.1: Drawing samples -  5%\n",
    "\n",
    "Use the given function `lab.generate_gaussian_clusters()` to create synthetic data samples in a 2D feature space. The dataset should be drawn from 5 Gaussians, each corresponding to a cluster. There are 4 classes; three classes correspond to one cluster only, whereas one class (class 1) corresponds to two clusters. The clusters are defined in the following table:\n",
    "\n",
    "| Cluster | Center x | Center y | Angle  | Variance x | Variance y | Samples | Class |\n",
    "|-|-|-|-|-|-|-|-|\n",
    "| __1__ | 40 | 35 | 100 | 20 | 10 | 550 | 0 |\n",
    "| __2__ | 55 | 190 | 45 | 25 | 15 | 450 | 1 |\n",
    "| __3__ | 180 | 40 | 125 | 25 | 20 | 350 | 1 |\n",
    "| __4__ | 120 | 120 | 45 | 35 | 20 | 500 | 2 |\n",
    "| __5__ | 200 | 200 | 120 | 30 | 15 | 500| 3 |\n",
    "\n",
    "__Modify__ the variable `clusters` according to the table. Read the documentation of the function in __lab.py__ to get further information about the arguments and the return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [] # To be modified!\n",
    "\n",
    "samples = lab.generate_gaussian_clusters(clusters)\n",
    "print('Number of generated samples N =', samples.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Splitting the data into a training set and a test set - 10%\n",
    "\n",
    "__Implement__ the function below, which takes the $N$ `samples` as well as the the ratio $r_{test} = N_{test}/N$ of training samples and returns:\n",
    "\n",
    "- $X$: training set features as array with shape $(N_{train}\\times 2)$\n",
    "- $y$: training set labels as array with shape $(N_{train})$ \n",
    "- $X_t$: test set features as array with shape $(N_{test}\\times 2)$\n",
    "- $y_t$: test set labels as array with shape$ (N_{test})$\n",
    "\n",
    "where $N_{train} = N\\cdot(1-r_{test})$ is the number of training samples and $N_{test} = N\\cdot r_{test}$ is the number of samples for testing. Make sure to __shuffle__ the samples randomly (e.g. using `np.random.shuffle()`)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(samples, r_test):\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "\n",
    "    X   =     # training set features\n",
    "    y   =     # training set labels\n",
    "\n",
    "    X_t =     # test set features\n",
    "    y_t =     # test set labels\n",
    "    \n",
    "    return X, y, X_t, y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell. It will use your function to generate and visualize the sets for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_t, y_t = split_train_test(samples, 0.5)\n",
    "\n",
    "# Plot both sets:\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize, PlotSize]  \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=CMAP)\n",
    "plt.scatter(X_t[:, 0], X_t[:, 1], c=y_t, edgecolors='k', cmap=CMAP, marker='v')\n",
    "plt.xlim((0, 255)); plt.ylim((0, 255))\n",
    "plt.title('Training samples (circles), Test samples (triangles)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run__ the next cell to fit a single Gaussian model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier instance\n",
    "ndc = NormalDistributionClassifier(num_classes=4, num_features=2)\n",
    "\n",
    "# Train the classifier\n",
    "ndc.fit(X, y)\n",
    "\n",
    "# Plot datasets\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=CMAP)\n",
    "plt.scatter(X_t[:, 0], X_t[:, 1], c=y_t, edgecolors='k', cmap=CMAP, marker='v')\n",
    "\n",
    "# Plot confidence ellipses: 0.5 sigma, 1 sigma, 1.5 sigma\n",
    "lab.plot_sigma_ellipses(ndc, CMAP, sigmas = [0.5, 1, 1.5])\n",
    "\n",
    "plt.xlim((0, 255)); plt.ylim((0, 255))\n",
    "plt.title('Training samples (circles), Test samples (triangles)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the likelihoods for each class can be visualized by computing the likelihood for each feature vector on a $255 \\times 255$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid set of 'all' features in the limits\n",
    "xx, yy = np.meshgrid(np.arange(0, 256, 1), np.arange(0, 256, 1))\n",
    "mesh_features = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Get likelihoods for the meshgrid samples\n",
    "L = ndc.compute_likelihoods(mesh_features)\n",
    "lab.print_probabilities(L, (256, 256), 'Likelihood', n_cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of visually analyzing a classifier is to plot the decision boundaries. In the next cell the whole feature space will be classified according to ML classification, which implicitly shows the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ndc.predict(mesh_features)\n",
    "lab.print_decision_boundaries(C, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering prior information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, assume that the prior probability for class 1 is 62 times higher than for the other classes. __Note__ that $P_{prior}$ must sum up to 1, to be a valid distribution. Run the cell and make a sanity-check of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_prior = # Assume that class 1 is 62 more probable. To be modified! (The distribution should sum up to 1)\n",
    "\n",
    "# Get posteriors for the new test samples\n",
    "P_post = ndc.compute_posteriors(L, P_prior)\n",
    "lab.print_probabilities(P_post, (256, 256), 'Posterior probability', n_cls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Mixture of Gaussian Model -  5%\n",
    "\n",
    "__Run__ the next cell trains a Gaussian Mixture Model to the training set of the toy dataset of each class. The list `N_clusters` defines the number of components per class. \n",
    "\n",
    "__Modify__ this list in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_clusters = # To be modified! \n",
    "\n",
    "# Instantiating the classifier\n",
    "gmm = lab.GaussianMixtureClassifier(N_clusters)\n",
    "\n",
    "# Train the classifier using EM:\n",
    "gmm.fit(X, y)   \n",
    "\n",
    "# Plot datasets\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=CMAP)\n",
    "plt.scatter(X_t[:, 0], X_t[:, 1], c=y_t, edgecolors='k', cmap=CMAP, marker='v')\n",
    "\n",
    "# Plot confidence ellipses: 0.5 sigma, 1 sigma, 1.5 sigma\n",
    "gmm.plot_sigma_ellipses(CMAP, sigmas = [0.5, 1, 1.5]) \n",
    "\n",
    "plt.xlim((0, 255)); plt.ylim((0, 255)); \n",
    "plt.title('Training samples (circles), Test samples (triangles)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells will again compute and show the likelihoods, posteriors for the complete feature space and plot the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get likelihoods for the meshgrid samples\n",
    "L = gmm.compute_likelihoods(mesh_features)\n",
    "lab.print_probabilities(L, (256, 256), 'Likelihood', n_cls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_prior = # Assume that class 1 is 62 more probable. To be modified! (The distribution should sum up to 1)\n",
    "\n",
    "# Get posteriors for the new test samples\n",
    "P_post = gmm.likelihoods_to_posteriors(L, P_prior)\n",
    "lab.print_probabilities(P_post, (256, 256), 'Posterior probability', n_cls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries\n",
    "C = gmm.predict(mesh_features) # this will assume a uniform prior\n",
    "lab.print_decision_boundaries(C, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4: Comparison and evaluation -  15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Print__ the likelihood, the posterior and the predicted class for the feature vector $X_{140-90} = (140, 90)$ using the NDC and GMM (assuming a uniform prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write__ a brief discussion, which answers the following questions:\n",
    "\n",
    "- Based on the prediction of the feature $X_{140-90}$, discuss the models performance. Which model achieves better results and why? \n",
    "\n",
    "- What happens when the number of components of a GMM is chosen to high / to low? Try and document different variants with respect to the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "*Write the discussion here. Do not forget to answer all questions, item by item, and to identify which answer belongs to which question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
